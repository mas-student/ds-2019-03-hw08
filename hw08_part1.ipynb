{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/c/mercari-price-suggestion-challenge \n",
    "# https://www.kaggle.com/c/home-credit-default-risk\n",
    "\n",
    "Выбрать одно из двух соревнований. Выбирайте по данным, с которыми вам интереснее работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-206578bd8897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import time\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, LabelEncoder, MinMaxScaler,  Imputer, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Ансамбли\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используйте параметр nrows, чтобы уменьшить выборку и сделать базовый разведочный анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/train.tsv', nrows=999999, sep='\\t')\n",
    "# mercari_prices = pd.read_csv('data/train.tsv', nrows=249, sep='\\t')\n",
    "mercari_prices = pd.read_csv('input/train.tsv', nrows=9999, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def prepare_data(N, subcats=True):\n",
    "#     df = pd.read_csv('input/train.tsv', nrows=N, sep='\\t')\n",
    "#     if subcats:\n",
    "#         cats = df.category_name.str.split('/', expand=True)\n",
    "#         df['cat1'] = cats[0]\n",
    "#         df['cat2'] = cats[1]\n",
    "#         df['cat3'] = cats[2]\n",
    "#         df = df.drop(['category_name'], axis=1)\n",
    "# #     df.head()\n",
    "\n",
    "#     df['brand_id'] = pd.Categorical(df['brand_name']).codes\n",
    "\n",
    "#     return df\n",
    "\n",
    "def load_data(N):\n",
    "    df = pd.read_csv('input/train.tsv', nrows=N, sep='\\t')\n",
    "#     if subcats:\n",
    "#         cats = df.category_name.str.split('/', expand=True)\n",
    "#         df['cat1'] = cats[0]\n",
    "#         df['cat2'] = cats[1]\n",
    "#         df['cat3'] = cats[2]\n",
    "#         df = df.drop(['category_name'], axis=1)\n",
    "# #     df.head()\n",
    "\n",
    "#     df['brand_id'] = pd.Categorical(df['brand_name']).codes\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# df = mercari_prices.copy()\n",
    "# df.head()\n",
    "\n",
    "# # # df.category_name.str.split('/')\n",
    "# # for category in df.category_name.str.split('/').apply(pd.Series).stack().reset_index(drop=True):\n",
    "# #     df[category] = df.category_name.str.contains(category)\n",
    "# # df = df.drop(['category_name'], axis=1)\n",
    "\n",
    "# # df.category_name.str.split('/', expand=True)\n",
    "\n",
    "# # # df.head().T\n",
    "# # # df.category_name.str.split('/', expand=True)\n",
    "# # # df.\n",
    "# # # df['categories'] = pd.Series(df.category_name.str.split('/', expand=True))\n",
    "# cats = df.category_name.str.split('/', expand=True)\n",
    "# df['cat1'] = cats[0]\n",
    "# df['cat2'] = cats[1]\n",
    "# df['cat3'] = cats[2]\n",
    "# df = df.drop(['category_name'], axis=1)\n",
    "# df.head()\n",
    "\n",
    "# df = prepare_data(17000)\n",
    "# df = prepare_data(9999)\n",
    "df = load_data(9999)\n",
    "\n",
    "df[df.item_description.str.contains('\\d', regex=True)][:10]\n",
    "df[df.name.str.contains('\\d', regex=True)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('brand_name').min()['price'][:10].plot.bar()\n",
    "# df.groupby('brand_name').mean()['price'][:10].plot.bar()\n",
    "# df.groupby('brand_name').max()['price'][:10].plot.bar()\n",
    "\n",
    "df['count'] = 1\n",
    "brands = list(df.groupby('brand_name').count()['count'].nlargest(10).index)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,4))\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# # # sns.violinplot(x='part', y='rating', order=parts, data=df)\n",
    "# sns.violinplot(x='brand_name', y='price', data=df[df['brand_name'].isin(brands)], ax=ax)\n",
    "# sns.despine(left=True)\n",
    "\n",
    "df.loc[:, ['brand_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head input/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('input/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Сделать baseline submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 1.1 Исследование признак price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.describe(percentiles=[.05, .25, .5, .75, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "df1 = load_data(9999)\n",
    "s1 = df1['price'] # .hist()\n",
    "s1.hist(ax=ax[0])  # (bins=100)\n",
    "\n",
    "s2 = df1[df1['price'] < 94.5]['price'] # .hist()\n",
    "s2.hist(ax=ax[1])  # (bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Исследование признака price в зависимости от brand_name или других признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# corr_matrix = df.corr()\n",
    "# display(corr_matrix)\n",
    "# df.drop(['brand_name', 'price'], axis=1).corr(method='pearson').style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)\n",
    "df['brand_id'] = pd.Categorical(df['brand_name']).codes\n",
    "df.dropna().corr(method='pearson').style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Реализовать цикл анализа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 признаки -> модель -> настройка параметров -> лучшая модель и ее значение метрики качества на кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "models = {\n",
    "    'log_reg': LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000),\n",
    "    'svc': SVC(kernel='linear', C=100),\n",
    "    'tree': DecisionTreeClassifier(),\n",
    "    'knei_reg': KNeighborsRegressor(),\n",
    "    'xgb': xgb.XGBClassifier(),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class Ensemble(BaseEstimator):  \n",
    "    \"\"\"An example of classifier\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models = list(models.values())\n",
    "    \n",
    "#     def __init__(self, models):\n",
    "# #         print('INIT', name)\n",
    "#         self.models = models\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        ___ = [\n",
    "            int(np.mean(items))\n",
    "            for items in \n",
    "            zip(*[model.predict(X) for model in self.models])\n",
    "        ]\n",
    "#         print('___', ___)\n",
    "        return ___\n",
    "       \n",
    "    def score(self, X, y=None):\n",
    "        return(sum(self.predict(X)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def prepare_data(N, subcats=True):\n",
    "    df = pd.read_csv('input/train.tsv', nrows=N, sep='\\t')\n",
    "    return df\n",
    "\n",
    "_models = models.copy()\n",
    "_models['ensemble'] = Ensemble()\n",
    "\n",
    "class Router(BaseEstimator):  \n",
    "#     \"\"\"An example of classifier\"\"\"\n",
    "\n",
    "    def __init__(self, name=None, estimator=None, params={}, x_pred=None, y_true=None, **kwargs):\n",
    "#         print('INIT', name)\n",
    "        self.name = name or 'svc' # type(models[model])\n",
    "        self.estimator = estimator\n",
    "        self.x_pred = x_pred\n",
    "        self.y_true = y_true\n",
    "        self.params = params  # or {}\n",
    "#         self.model_instance = models[self.name]\n",
    "        self.initial_params = _models[self.name].get_params()\n",
    "#         print('PARAMS 1', name, self.name, self.model_instance, self.initial_params)\n",
    "#         params.update()\n",
    "        self.model_instance = type(_models[self.name])(**self.params)\n",
    "    \n",
    "#     def get_model_instance(self):\n",
    "#         return type(self.model_instance)(**self.params)\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    @name.setter\n",
    "    def name(self, value):\n",
    "#         print('SET NAME', value)\n",
    "        self._name = value\n",
    "        params = _models[self.name].get_params()  # getattr(self, 'initial_params', {}).copy() \n",
    "        params.update(getattr(self, 'params', {}))\n",
    "#         print('PARAMS 2', value, getattr(self, 'name', None), getattr(self, 'model_instance', None), params)\n",
    "        self.model_instance = type(_models[self._name])(**params)\n",
    "#         self.model_instance = type(_models[self._name])(**getattr(self, 'params', {}))\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return self._params\n",
    "    \n",
    "    @params.setter\n",
    "    def params(self, value):\n",
    "#         print('SET PARAMS', value)\n",
    "        self._params = value\n",
    "        params = getattr(self, 'initial_params', {}).copy()\n",
    "        params.update(getattr(self, 'params', {}))\n",
    "#         print('PARAMS 3', value, getattr(self, 'name', None), getattr(self, 'model_instance', None), params)\n",
    "        self.model_instance = type(_models[self._name])(**params)\n",
    "#         self.model_instance = type(_models[self._name])(**getattr(self, 'params', {}))\n",
    "# #         print('SET PARAMS', value)\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "#         self.get_model_instance().fit(X, y)\n",
    "#         return self\n",
    "#         self.model_instance = models[self.name]\n",
    "        self.model_instance.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "#         return self.get_model_instance().predict(X)\n",
    "#         self.model_instance = models[self.name]\n",
    "        return list(map(int, self.model_instance.predict(X)))\n",
    "       \n",
    "    def score(self, X, y=None):\n",
    "#         print('SCORE', self.model_instance.predict(X).shape, y.shape)\n",
    "        error = mean_squared_error(self.model_instance.predict(X), y)\n",
    "        score = 1 / (1 + error)\n",
    "#         print(self.name, score, error)\n",
    "#         print('SCORE', self.name, score, self.model_instance.score(X, y))\n",
    "        print('.', end='')\n",
    "        return score\n",
    "    \n",
    "#         error = sum([1 if 0.1 < abs(y1 - y2) else 0 for y1, y2 in zip(self.model_instance.predict(X), y)])/y.shape[0]\n",
    "#         score = 1 / (1 + error)\n",
    "#         print(self.name, score, error)\n",
    "#         return score\n",
    "\n",
    "def category_splitter(df):\n",
    "    df = df[:]\n",
    "    cats = df.category_name.str.split('/', expand=True)\n",
    "#     top, middle, bottom = df.category_name.str.split('/', expand=True)\n",
    "#     df = df.drop(['category_name'], axis=1)\n",
    "    \n",
    "#     df = pd.DataFrame(dict(**df.to_dict(), cat1=cats[0], cat2=cats[1], cat3=cats[2]))\n",
    "    df = pd.DataFrame(dict(**df.to_dict(), top=cats[0], middle=cats[1], bottom=cats[2]))\n",
    "#     df = pd.DataFrame(dict(**df.to_dict(), top=top, middle=middle, bottom=bottom))\n",
    "    \n",
    "    return df\n",
    "# category_splitter = completor\n",
    "completor = category_splitter\n",
    "\n",
    "def _execute_analyze_cycle(model, dataframe, *, x=None, y=None, params=None, features=None, preparers=None, matched=False):\n",
    "    preparers = preparers or []\n",
    "    ret = dict()\n",
    "    \n",
    "    def feature_selector(x, features=None):\n",
    "        if features:\n",
    "            x = x.loc[:, features.split(',')]\n",
    "        return x\n",
    "    \n",
    "    dataframe = dataframe.dropna()\n",
    "    \n",
    "    x = x or dataframe.drop(['price'], axis=1)\n",
    "    y = y or np.ravel(dataframe.loc[:, ['price']])\n",
    "\n",
    "\n",
    "    preparation = Pipeline(steps = [\n",
    "        ('feature_selector', FunctionTransformer(feature_selector, validate=False)),\n",
    "\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "        ('scaler', StandardScaler()),            \n",
    "    ])\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x,\n",
    "        y,\n",
    "        test_size=0.20, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    y_true = np.ravel(y_test)\n",
    "#     y_pred = search_cv.predict(x_test)\n",
    "\n",
    "\n",
    "    _x_train, _x_test, _y_train, _y_test = train_test_split(\n",
    "        preparation.fit_transform(x),\n",
    "        y,\n",
    "        test_size=0.20, \n",
    "        random_state=42\n",
    "    )\n",
    "    _y_true = np.ravel(_y_test)\n",
    "\n",
    "\n",
    "    def estimator(model):\n",
    "        print(model)\n",
    "#         x = preparation.fit_transform(x_test)\n",
    "        model.fit(_x_train, _y_train)\n",
    "#         print(model.predict(x).shape, y_true.shape)\n",
    "        return accuracy_score(y_pred=list(map(int, model.predict(_x_test))), y_true=_y_true)\n",
    "\n",
    "#     pipeline = Pipeline(steps = [\n",
    "#         ('completor', FunctionTransformer(completor, validate=False)),\n",
    "# #             ('x_selector', FunctionTransformer(sample_selector, validate=False, )),\n",
    "# #             ('y_selector', FunctionTransformer(sample_selector, pass_y=True, validate=False, )),\n",
    "#         ('feature_selector', FunctionTransformer(feature_selector, validate=False)),\n",
    "\n",
    "#         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#         ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "#         ('scaler', StandardScaler()),            \n",
    "# #         ('model', models[model]) if isinstance(model, str) else model,\n",
    "#         ('model', Router('log_reg', estimator=None, x_pred=x_test, y_true=y_true))\n",
    "#     ])\n",
    "\n",
    "    steps = [\n",
    "# #         ('completor', FunctionTransformer(completor, validate=False)),\n",
    "# #             ('x_selector', FunctionTransformer(sample_selector, validate=False, )),\n",
    "# #             ('y_selector', FunctionTransformer(sample_selector, pass_y=True, validate=False, )),\n",
    "#         ('feature_selector', FunctionTransformer(feature_selector, validate=False)),\n",
    "    ] + preparers + [\n",
    "#             ('x_selector', FunctionTransformer(sample_selector, validate=False, )),\n",
    "#             ('y_selector', FunctionTransformer(sample_selector, pass_y=True, validate=False, )),\n",
    "        ('feature_selector', FunctionTransformer(feature_selector, validate=False)),\n",
    "        \n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "        ('scaler', StandardScaler()),            \n",
    "#         ('model', models[model]) if isinstance(model, str) else model,\n",
    "        ('model', Router('log_reg', estimator=None, x_pred=x_test, y_true=y_true))\n",
    "    ]\n",
    "#     print('PS', preparers)\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    grid = params or {}\n",
    "\n",
    "#     search_cv = RandomizedSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    search_cv = GridSearchCV(pipeline, grid, cv=3, iid=True)             \n",
    "    search_cv.fit(x_train, np.array(y_train))\n",
    "\n",
    "    ret['search_cv'] = search_cv\n",
    "\n",
    "    y_pred = search_cv.predict(x_test)\n",
    "    \n",
    "#     print('PRED', y_pred)\n",
    "#     print('TRUE', y_true)\n",
    "\n",
    "    ret['accuracy_score'] = 0  # accuracy_score(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "grid0 = {\n",
    "# #     'model__penalty': ['l2', 'none'],\n",
    "#     'model__max_iter': [250], # [100, 250, 500, 1000],\n",
    "    \n",
    "#     'x_selector__kw_args': [{'count': count} for count in [250, 500, 750]],\n",
    "#     'y_selector__kw_args': [{'count': count} for count in [250, 500, 750]],\n",
    "    'feature_selector__kw_args': [\n",
    "#         {'features': 'brand_name'}, \n",
    "#         {'features': 'cat1'}, \n",
    "#         {'features': 'cat2'}, \n",
    "#         {'features': 'cat3'},\n",
    "#         {'features': 'cat1,cat2,cat3'},\n",
    "        {'features': 'category_name'},\n",
    "#         {'features': 'shipping'},\n",
    "    ],\n",
    "    'model__name': list(_models.keys()),\n",
    "#     'model__name': ['knei_reg'], # list(models.keys()),  # ['log_reg', 'svc', 'tree'], # [100, 250, 500, 1000],\n",
    "# #     'model__params': [{'C': C} for C in [1, 10, 100, 1000, 10000]],\n",
    "#     'model__params': [{'n_neighbors': param} for param in range(2, 6)],\n",
    "# #     'model__params': [{'max_iter': param} for param in range(100, 1000, 100)],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "grid = {\n",
    "# #     'model__penalty': ['l2', 'none'],\n",
    "#     'model__max_iter': [250], # [100, 250, 500, 1000],\n",
    "    \n",
    "#     'x_selector__kw_args': [{'count': count} for count in [250, 500, 750]],\n",
    "#     'y_selector__kw_args': [{'count': count} for count in [250, 500, 750]],\n",
    "    'feature_selector__kw_args': [\n",
    "#         {'features': 'brand_name'}, \n",
    "        \n",
    "#         {'features': 'cat1'}, \n",
    "#         {'features': 'cat2'}, \n",
    "#         {'features': 'cat3'},\n",
    "#         {'features': 'cat1,cat2,cat3'},\n",
    "#         {'features': 'category_name'},\n",
    "        \n",
    "#         {'features': 'top'}, \n",
    "#         {'features': 'middle'}, \n",
    "#         {'features': 'bottom'},\n",
    "#         {'features': 'top,middle,bottom'},\n",
    "#         {'features': 'category_name'},\n",
    "\n",
    "#         {'features': 'shipping'},\n",
    "        \n",
    "        {'features': 'middle'},\n",
    "    ],\n",
    "    'model__name': list(_models.keys()),\n",
    "#     'model__name': ['knei_reg'], # list(models.keys()),  # ['log_reg', 'svc', 'tree'], # [100, 250, 500, 1000],\n",
    "# #     'model__params': [{'C': C} for C in [1, 10, 100, 1000, 10000]],\n",
    "#     'model__params': [{'n_neighbors': param} for param in range(2, 6)],\n",
    "# #     'model__params': [{'max_iter': param} for param in range(100, 1000, 100)],\n",
    "}\n",
    "\n",
    "# SVC(kernel='linear', C=100)\n",
    "\n",
    "preparers = [\n",
    "    ('category_splitter', FunctionTransformer(category_splitter, validate=False))\n",
    "]\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "# def extract_target_into_x(data, target):\n",
    "# #     pd.DataFrame(data['search_cv'].cv_results_)\n",
    "\n",
    "# #     data = data['search_cv'].cv_results_\n",
    "\n",
    "#     data\n",
    "\n",
    "# #     params = list(dict(data)['params'])[0]\n",
    "# #     param_name = 'param_' + list(params.keys())[0]\n",
    "# #     param_name\n",
    "\n",
    "#     # # data = {k:[(list(vv.values())[0] if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "#     # data = {k:[(list(','.join(vv.values())) if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "#     data = {k:[(','.join(list(vv.values())) if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "\n",
    "#     data = pd.DataFrame(data)\n",
    "#     data\n",
    "\n",
    "# #     params.pop('model__name')\n",
    "\n",
    "#     data['x'] = data[target]\n",
    "\n",
    "    # data.groupby(['param_model__name', param_name]).sum()['mean_test_score'].unstack().plot.bar()    \n",
    "\n",
    "# # print('BEGIN', end='')\n",
    "# # ret0 = _execute_analyze_cycle('log_reg', prepare_data(300), params=grid0)\n",
    "# # print('END')\n",
    "# print('BEGIN', end='')\n",
    "# ret = _execute_analyze_cycle('log_reg', prepare_data(300), params=grid, preparers=preparers)\n",
    "# print('END')\n",
    "# # data = ret\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "def extract_target_from_search_cv_into_x(data, target):\n",
    "    data = data['search_cv'].cv_results_\n",
    "    data = {k:[(','.join(list(vv.values())) if k == target else vv) for vv in v] for k, v in data.items()}\n",
    "    data = pd.DataFrame(data)\n",
    "    data['x'] = data[target]\n",
    "    \n",
    "    return data\n",
    "\n",
    "     \n",
    "# # (extract_target_from_search_cv_into_x(ret0, 'param_feature_selector__kw_args')\n",
    "# #  .groupby(['param_model__name', 'x']).sum()['mean_test_score'].unstack().plot.bar(ax=ax[0]))\n",
    "# (extract_target_from_search_cv_into_x(ret, 'param_feature_selector__kw_args')\n",
    "#  .groupby(['param_model__name', 'x']).sum()['mean_test_score'].unstack()\n",
    "# #  .loc[:, ['category_name', 'cat1,cat2,cat3', 'cat1', 'cat2', 'cat3']]\n",
    "#  .loc[:, ['category_name', 'top,middle,bottom', 'top', 'middle', 'bottom']]\n",
    "#  .plot.bar(ax=ax))\n",
    "     \n",
    "# plt.legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pd.DataFrame(data['search_cv'].cv_results_)\n",
    "\n",
    "# data = data['search_cv'].cv_results_\n",
    "\n",
    "# data\n",
    "\n",
    "# params = list(dict(data)['params'])[0]\n",
    "# param_name = 'param_' + list(params.keys())[0]\n",
    "# param_name\n",
    "\n",
    "# # # data = {k:[(list(vv.values())[0] if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "# # data = {k:[(list(','.join(vv.values())) if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "# data = {k:[(','.join(list(vv.values())) if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "\n",
    "# data = pd.DataFrame(data)\n",
    "# data\n",
    "\n",
    "# params.pop('model__name')\n",
    "\n",
    "# data.groupby(['param_model__name', param_name]).sum()['mean_test_score'].unstack().plot.bar()\n",
    "# plt.legend(loc='best')\n",
    "\n",
    "def __execute_analyze_cycle(model=None, features=None, preparers=None, preparer=None):\n",
    "    preparers = [] if preparer is None else [\n",
    "        (preparer.__name__, FunctionTransformer(preparer, validate=False))\n",
    "#         ('category_splitter', FunctionTransformer(category_splitter, validate=False))\n",
    "    ]\n",
    "    x='param_feature_selector__kw_args'\n",
    "    df = prepare_data(300)\n",
    "    models = model if model is not None else list(_models.keys())\n",
    "    features = features if features is not None else df.columns\n",
    "    preparers = preparers or []\n",
    "    grid = {    \n",
    "        'feature_selector__kw_args': [{'features': feature} for feature in features],\n",
    "        'model__name': models,\n",
    "    }\n",
    "    ret = _execute_analyze_cycle('log_reg', df, params=grid, preparers=preparers)\n",
    "    ret = extract_target_from_search_cv_into_x(ret, x)\n",
    "    ret['features'] = ret['param_feature_selector__kw_args']\n",
    "    ret['models'] = ret['param_model__name']\n",
    "    ret['score'] = ret['mean_test_score']\n",
    "#     ret = ret.loc[:, features]\n",
    "    return ret\n",
    "\n",
    "def ___execute_analyze_cycle(models=None, N=None, **kwargs):\n",
    "#     all_preparers = [\n",
    "#         (k, FunctionTransformer(v, validate=False)) for k, v in kwargs.items() \n",
    "#     ]\n",
    "    preparers = []\n",
    "    bodies = []\n",
    "#     print('KW', type(kwargs), list(kwargs.items()))\n",
    "    for name, body in kwargs.items():\n",
    "#         print('???', name, body)\n",
    "        if body not in bodies:\n",
    "            bodies.append(body)\n",
    "            preparers.append((name, FunctionTransformer(body, validate=False)))\n",
    "#         else:\n",
    "#             print('!', bodies, body)\n",
    "#     preparers = [] if preparer is None else [\n",
    "#         (preparer.__name__, FunctionTransformer(preparer, validate=False))\n",
    "# #         ('category_splitter', FunctionTransformer(category_splitter, validate=False))\n",
    "#     ]\n",
    "#     print('PS0', preparers)\n",
    "    x='param_feature_selector__kw_args'\n",
    "    df = prepare_data(N or 300)\n",
    "    models = models if models is not None else list(_models.keys())\n",
    "    features = list(kwargs.keys()) # features if features is not None else df.columns\n",
    "#     preparers = preparers or []\n",
    "    grid = {    \n",
    "        'feature_selector__kw_args': [{'features': feature} for feature in features],\n",
    "        'model__name': models,\n",
    "    }\n",
    "    ret = _execute_analyze_cycle('log_reg', df, params=grid, preparers=preparers)\n",
    "    ret = extract_target_from_search_cv_into_x(ret, x)\n",
    "#     ret['features'] = ret['param_feature_selector__kw_args']\n",
    "#     ret['models'] = ret['param_model__name']\n",
    "#     ret['score'] = ret['mean_test_score']\n",
    "    ret = ret.rename(columns=dict(\n",
    "        param_feature_selector__kw_args='features',\n",
    "        param_model__name='models',\n",
    "        mean_test_score='score',\n",
    "    ))\n",
    "    print(ret.columns)\n",
    "#     ret = ret.loc[:, features]\n",
    "    ret = ret.groupby(['models', 'features']).sum()['score'].unstack().loc[:, features]\n",
    "    return ret\n",
    "\n",
    "\n",
    "     \n",
    "# features = ['category_name', 'bottom']\n",
    "# (__execute_analyze_cycle(features=features, preparer=category_splitter)\n",
    "# #  .groupby(['param_model__name', 'features']).sum()['mean_test_score'].unstack()\n",
    "#   .groupby(['models', 'features']).sum()['mean_test_score'].unstack() \n",
    "# #  .loc[:, ['category_name', 'cat1,cat2,cat3', 'cat1', 'cat2', 'cat3']]\n",
    "# #  .loc[:, ['category_name', 'top,middle,bottom', 'top', 'middle', 'bottom']]\n",
    "#  .loc[:, features]\n",
    "#  .plot.bar(ax=ax[0]))\n",
    "\n",
    "     \n",
    "def category_splitter(df):\n",
    "#     if 'top' in df.columns:\n",
    "#         return df\n",
    "    \n",
    "#     cats = df.category_name.str.split('/', expand=True)\n",
    "    top, middle, bottom = df.category_name.str.split('/', expand=True).to_dict().values()\n",
    "#     print(cats)\n",
    "#     print(t,m,b)\n",
    "#     return pd.DataFrame(dict(**df.to_dict(), top=cats[0], middle=cats[1], bottom=cats[2]))\n",
    "    return pd.DataFrame(dict(**df.to_dict(), top=top, middle=middle, bottom=bottom))\n",
    "    \n",
    "def first(df):\n",
    "    return pd.DataFrame(dict(**df.to_dict(), first=df.category_name.str[0]))\n",
    "\n",
    "cs = category_splitter\n",
    "___execute_analyze_cycle(N=200, top=cs, middle=cs, bottom=cs, first=first).plot.bar(ax=ax[1])\n",
    "    \n",
    "# def first(df):\n",
    "#     return pd.DataFrame(dict(**df.to_dict(), first=df.category_name.str[0]))\n",
    "\n",
    "# features = ['first',]\n",
    "# (__execute_analyze_cycle(features=features, preparer=first)\n",
    "#  .groupby(['models', 'features']).sum()['score'].unstack() \n",
    "#  .loc[:, features]\n",
    "#  .plot.bar(ax=ax[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = prepare_data(50000)\n",
    "df['number'] = df.name.str.extract(r\"(\\d+)\")\n",
    "\n",
    "df[df['number'].notna()].loc[:, ['number', 'name', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def extract_target_from_search_cv_into_x(data, target):\n",
    "# #     pd.DataFrame(data['search_cv'].cv_results_)\n",
    "\n",
    "#     data = data['search_cv'].cv_results_\n",
    "\n",
    "#     data\n",
    "\n",
    "# #     params = list(dict(data)['params'])[0]\n",
    "# #     param_name = 'param_' + list(params.keys())[0]\n",
    "# #     param_name\n",
    "\n",
    "#     # # data = {k:[(list(vv.values())[0] if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "#     # data = {k:[(list(','.join(vv.values())) if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "#     data = {k:[(','.join(list(vv.values())) if k == target else vv) for vv in v] for k, v in data.items()}\n",
    "\n",
    "#     data = pd.DataFrame(data)\n",
    "\n",
    "# #     params.pop('model__name')\n",
    "\n",
    "#     data['x'] = data[target]\n",
    "    \n",
    "#     return data\n",
    "\n",
    "\n",
    "# (extract_target_from_search_cv_into_x(ret, 'param_feature_selector__kw_args')\n",
    "#  .groupby(['param_model__name', 'x']) .sum()['mean_test_score'].unstack().plot.bar())\n",
    "\n",
    "\n",
    "# # data = ret\n",
    "\n",
    "# # pd.DataFrame(data['search_cv'].cv_results_)\n",
    "\n",
    "# # data = data['search_cv'].cv_results_\n",
    "\n",
    "# # # data\n",
    "\n",
    "# # params = list(dict(data)['params'])[0]\n",
    "# # data\n",
    "# # params\n",
    "# # # param_name = 'param_' + list(params.keys())[0]\n",
    "# # # param_name\n",
    "\n",
    "# # def merge(key, values):\n",
    "# #     if key == param_name:\n",
    "# #         print('VALUES', values)\n",
    "# #         return ','.join(values)\n",
    "# #     else:\n",
    "# #         return values\n",
    "\n",
    "# # # # data = {k:[(list(vv.values())[0] if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "# # # data = {k:[merge(k, (list(vv.values())) if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "# # data = {k:[(','.join(list(vv.values())) if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "\n",
    "# # data\n",
    "# # data = pd.DataFrame(data)\n",
    "# # # data\n",
    "\n",
    "# # # params.pop('model__name')\n",
    "\n",
    "\n",
    "# # data.groupby(['param_model__name', param_name]) .sum()['mean_test_score'].unstack().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def prepare_data(N, subcats=True):\n",
    "#     df = pd.read_csv('input/train.tsv', nrows=N, sep='\\t')\n",
    "# #     if subcats:\n",
    "# #         cats = df.category_name.str.split('/', expand=True)\n",
    "# #         df['cat1'] = cats[0]\n",
    "# #         df['cat2'] = cats[1]\n",
    "# #         df['cat3'] = cats[2]\n",
    "# #         df = df.drop(['category_name'], axis=1)\n",
    "# # #     df.head()\n",
    "\n",
    "# #     df['brand_id'] = pd.Categorical(df['brand_name']).codes\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "# # na = df[features].isna().apply(lambda s: s.value_counts(), axis=0)\n",
    "# # na\n",
    "# # list(filter(lambda t: t[0] > 0, map(lambda col: (na.loc[True, col], col), na.columns)))\n",
    "\n",
    "\n",
    "# # df = mercari_prices.dropna()\n",
    "\n",
    "\n",
    "# # # X_train, y_train = df.drop(['price'], axis=1), df.loc[:, ['price']]\n",
    "\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # preparation = Pipeline(steps = [\n",
    "# #     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "# #     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "# #     ('scaler',StandardScaler()),\n",
    "# # ])\n",
    "\n",
    "# # x_train, x_test, y_train, y_test = train_test_split(\n",
    "# #     preparation.fit_transform(df.drop(['price'], axis=1)), \n",
    "# #     np.ravel(df.loc[:, ['price']]),\n",
    "# #     test_size=0.20, \n",
    "# #     random_state=42\n",
    "# # )\n",
    "\n",
    "\n",
    "# # # x_train, x_test, y_train, y_test = train_test_split(\n",
    "# # #     df.drop(['price'], axis=1), \n",
    "# # #     np.ravel(df.loc[:, ['price']]),\n",
    "# # #     test_size=0.20, \n",
    "# # #     random_state=42\n",
    "# # # )\n",
    "    \n",
    "# # display('SHAPE', mercari_prices.shape, df.shape)\n",
    "# # x_train, y_train\n",
    "# pass\n",
    "\n",
    "# def execute_analyze_cycle(models, dataframe, *, x=None, y=None, params=None, features=None, matched=False):\n",
    "#     ret = dict()\n",
    "    \n",
    "# #     def sample_selector(x, count=None, *args, **kwargs):\n",
    "# # #         print('SELECTOR', x, args, kwargs)\n",
    "# # #         print('SELECTOR', x, features)\n",
    "# #         print('SAMPLE', x[:10], count, args, kwargs)\n",
    "# # #         if count:\n",
    "# # #             x = x.iloc[:count]\n",
    "# # #         if count:\n",
    "# # #             y = y[:count]\n",
    "# #         return x\n",
    "        \n",
    "# #     def selector(X, *args, **kwargs):\n",
    "#     def feature_selector(x, features=None):\n",
    "# #         print('SELECTOR', x, args, kwargs)\n",
    "# #         print('SELECTOR', x, features)\n",
    "#         print('FEATURE', type(x), features)\n",
    "#         if features:\n",
    "#             x = x.loc[:, features]\n",
    "#         return x\n",
    "\n",
    "    \n",
    "#     selection = Pipeline(steps = [\n",
    "#         ('feature_selector', FunctionTransformer(feature_selector, validate=False)),\n",
    "#     ])\n",
    "        \n",
    "#     preparation = Pipeline(steps = [\n",
    "# #         ('selector', FunctionTransformer(selector, validate=False)),\n",
    "# #         ('selector', FunctionTransformer(None)),\n",
    "#         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#         ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "# #         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#     ])\n",
    "    \n",
    "\n",
    "#     if type(models) is list: \n",
    "#         models = {str(i): model for i, model in enumerate(models)}\n",
    "#     elif type(models) is dict:\n",
    "#         pass\n",
    "#     else:\n",
    "#         models = {'0': models}\n",
    "        \n",
    "#     predictions = []\n",
    "#     accuracy_scores = []\n",
    "    \n",
    "#     y_true = []\n",
    "        \n",
    "#     for name, model in models.items():\n",
    "#         x = x or dataframe.drop(['price'], axis=1)\n",
    "#         if type(features) is list:\n",
    "#             x = x.loc[:, features]\n",
    "#         elif type(features) is dict and name in features:\n",
    "#             not_found = [feature for feature in features[name] if feature not in x.columns]\n",
    "#             if not_found:\n",
    "#                 print('features {} not found in columns {}'.format(not_found, x.columns))\n",
    "#                 return\n",
    "#             x = x.loc[:, features[name]]\n",
    "# #             if name in x.columns:\n",
    "# #                 x = x.loc[:, features[name]]\n",
    "# #                 print('FS', features[name])\n",
    "# #             else:\n",
    "# #                 print('features {} not found in columns {}'.format(features[name], x.columns))\n",
    "#         y = y or np.ravel(dataframe.loc[:, ['price']])\n",
    "\n",
    "#     #     x = x.drop(['train_id'], axis=1)\n",
    "\n",
    "# #         print('SELECTED', selection.transform(x))\n",
    "    \n",
    "#         x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     #         x.drop(['train_id'], axis=1), \n",
    "#     #         preparation.fit_transform(x.drop(['train_id'], axis=1)), \n",
    "# #             preparation.fit_transform(x), \n",
    "#             x,\n",
    "# #             preparation.transform(x),\n",
    "# #             preparation.fit_transform(selection.transform(x)),\n",
    "#             y,\n",
    "#             test_size=0.20, \n",
    "#             random_state=42\n",
    "#         )\n",
    "\n",
    "#     #     ids_train, ids_test = x_train.loc[:, ['train_id']], x_test.loc[:, ['train_id']]\n",
    "\n",
    "#     #     ret['ids_train'] = ids_train\n",
    "#     #     ret['ids_test'] = ids_test\n",
    "#         ret['x_test'] = x_test\n",
    "#         ret['y_test'] = y_test\n",
    "\n",
    "#         y_true = np.ravel(y_test)\n",
    "\n",
    "#     #     x_train, x_test = map(preparation.fit_transform, (x_train, x_test))\n",
    "\n",
    "\n",
    "        \n",
    "# #         if type(features) is list:\n",
    "# #             x = x.loc[:, features]\n",
    "\n",
    "# #         def selector(*args, **kwargs):\n",
    "# #             print('SELECTOR', args, kwargs)\n",
    "# #             return []\n",
    "        \n",
    "#         pipeline = Pipeline(steps = [\n",
    "#         # #     ('imputer', SimpleImputer()),\n",
    "#         #     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "#         #     ('scaler', StandardScaler()),\n",
    "#         # #     ('pca', PCA(n_components=2)),\n",
    "#         # #         ('pca', PCA()),\n",
    "#         #     ('log_reg', log_reg),\n",
    "            \n",
    "# #             ('x_selector', FunctionTransformer(sample_selector, validate=False, )),\n",
    "# #             ('y_selector', FunctionTransformer(sample_selector, pass_y=True, validate=False, )),\n",
    "#             ('feature_selector', FunctionTransformer(feature_selector, validate=False)),\n",
    "            \n",
    "#         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#         ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "# #         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#         ('scaler', StandardScaler()),\n",
    "\n",
    "            \n",
    "            \n",
    "# #             ('selector', FunctionTransformer(selector)), \n",
    "#             ('model', model),\n",
    "#         ])\n",
    "\n",
    "#         grid = params or {}\n",
    "\n",
    "#     #     search_cv = RandomizedSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "#         search_cv = GridSearchCV(pipeline, grid, cv=3, iid=True)     \n",
    "\n",
    "# #     #     search_cv.fit(np.array(x_train.drop(['train_id'], axis=1)), np.array(y_train))\n",
    "# #         search_cv.fit(np.array(x_train), np.array(y_train))mv\n",
    "        \n",
    "# #         print('Y_TRAIN', y_train[:10])\n",
    "    \n",
    "#         search_cv.fit(x_train, np.array(y_train))\n",
    "\n",
    "#         search_cv.best_estimator_\n",
    "\n",
    "#         ret['search_cv'] = search_cv\n",
    "        \n",
    "#         ret['best_estimator'] = search_cv.best_estimator_\n",
    "        \n",
    "#         ret['best_score'] = search_cv.best_score_\n",
    "#     #     display(search_cv.best_score_)\n",
    "\n",
    "#     #     ret['accuracy_score'] = accuracy_score(y_pred = search_cv.predict(x_test), y_true = np.ravel(y_test))\n",
    "#         y_pred = search_cv.predict(x_test)\n",
    "#         predictions.append(y_pred)\n",
    "#         accuracy_scores.append(accuracy_score(y_pred=y_pred, y_true=y_true))\n",
    "        \n",
    "# #     y_true = np.ravel(y_test)\n",
    "# #     display(x_test.shape, y_pred.shape, y_true.shape)\n",
    "# #     display(type(y_pred == y_true), y_pred == y_true)\n",
    "# #     print(y_pred, y_true)\n",
    "#     from collections import Counter\n",
    "#     y_pred = [\n",
    "# #         list(Counter(items).keys())[0]\n",
    "# #         Counter(items).most_common()[0][0]\n",
    "#         int(np.mean(items))\n",
    "#         for items in \n",
    "#         zip(*predictions)]\n",
    "#     ret['y_pred'] = y_pred\n",
    "#     ret['y_true'] = y_true\n",
    "#     ret['accuracy_score'] = accuracy_score(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "# #     if matched:\n",
    "# #         display(df[df['train_id'].isin(list(ids_test[y_pred == y_true]['train_id']))])\n",
    "    \n",
    "# #     display(ret['accuracy_score'])\n",
    "    \n",
    "#     ret['accuracy_scores'] = accuracy_scores\n",
    "#     ret['predictions'] = predictions\n",
    "#     return ret\n",
    "\n",
    "# def execute_mutly_analyze_cycles(Ns, models, *args, trace=False, **kwargs):  # FIXME\n",
    "# # def execute_mutly_analyze_cycles(Ns, model, dataframe, *args, **kwargs):\n",
    "#     scores = pd.DataFrame({'size': [], 'best_score': [], 'accuracy_score': []})\n",
    "# # display(scores)\n",
    "# # for i, N in enumerate(range(1000, 10000, 1000)):\n",
    "# # for N in [3000, 3099, 3299, 3499, 3699, 3999, 4200]:\n",
    "# # for N in [3780, 3830, 3960, 3390, 3999, 4410]:\n",
    "# # for N in range(3780, 3810):\n",
    "#     for i, N in enumerate(Ns):\n",
    "#         if trace:\n",
    "#             print(N)\n",
    "\n",
    "#         dataframe = prepare_data(N, subcats=True)\n",
    "#         dataframe = dataframe.dropna()\n",
    "# #                 dataframe = dataframe.dropna()\n",
    "\n",
    "#         score = execute_analyze_cycle(models, dataframe, *args, **kwargs)\n",
    "#         if not score:\n",
    "#             continue\n",
    "\n",
    "#         scores.loc[i] = (N, score['best_score'], score['accuracy_score'])\n",
    "    \n",
    "#     return scores\n",
    "\n",
    "# # features_set= [\n",
    "# #     {'svc': ['brand_id']},\n",
    "# #     {'svc': ['brand_name']},\n",
    "# #     {'svc': ['name']},\n",
    "# #     {'svc': ['shipping']},\n",
    "# # ]\n",
    "# # models = {\n",
    "# #     'log_reg': LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)),\n",
    "# #     'svc': SVC(kernel='linear', C=100),\n",
    "# #     'tree': DecisionTreeClassifier(),\n",
    "# # }\n",
    "\n",
    "# Ns = [1333]\n",
    "# by_model_and_feature = pd.DataFrame(dict.fromkeys(['feature', 'model', 'accuracy_score'], []))\n",
    "# # features_set = [{'svc': [column]} for column in df.columns if column not in ['price']]\n",
    "# features_set = [{model[0]: [column] for model in models} for column in df.columns if column not in ['price']]\n",
    "\n",
    "\n",
    "# # for features in features_set:    \n",
    "# #     for model_name, model in models.items():\n",
    "# #         feature = features[model_name][0]\n",
    "# #         scores = execute_mutly_analyze_cycles(\n",
    "# #             Ns,\n",
    "# #     #                 range(250, 500, 250),\n",
    "# #             {model_name: model}, \n",
    "# #     #         features={'svc': ['brand_id']}\n",
    "# #             features=features\n",
    "# #         )\n",
    "# #         display(features, scores)\n",
    "# #         if scores.shape[0] > 0:\n",
    "# #             by_model_and_feature.loc[by_model_and_feature.shape[0]] = (feature, model_name, scores.iloc[0]['accuracy_score'])\n",
    "# # #         display(features, scores)\n",
    "# # display(by_model_and_feature)\n",
    "\n",
    "# # for features in features_set:\n",
    "# #     scores = execute_mutly_analyze_cycles(\n",
    "# #         Ns,\n",
    "# # #                 range(250, 500, 250),\n",
    "# #         {'svc': SVC(kernel='linear', C=100)}, \n",
    "# # #         features={'svc': ['brand_id']}\n",
    "# #         features=features\n",
    "# #     )\n",
    "# #     display(features, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# from sklearn.base import BaseEstimator\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# def prepare_data(N, subcats=True):\n",
    "#     df = pd.read_csv('input/train.tsv', nrows=N, sep='\\t')\n",
    "# #     if subcats:\n",
    "# #         cats = df.category_name.str.split('/', expand=True)\n",
    "# #         df['cat1'] = cats[0]\n",
    "# #         df['cat2'] = cats[1]\n",
    "# #         df['cat3'] = cats[2]\n",
    "# #         df = df.drop(['category_name'], axis=1)\n",
    "# # #     df.head()\n",
    "\n",
    "# #     df['brand_id'] = pd.Categorical(df['brand_name']).codes\n",
    "\n",
    "#     return df\n",
    "\n",
    "# _models = models.copy()\n",
    "# _models['ensemble'] = Ensemble()\n",
    "\n",
    "# #     from collections import Counter\n",
    "# #     y_pred = [\n",
    "# # #         list(Counter(items).keys())[0]\n",
    "# # #         Counter(items).most_common()[0][0]\n",
    "# #         int(np.mean(items))\n",
    "# #         for items in \n",
    "# #         zip(*predictions)]\n",
    "\n",
    "# class Router(BaseEstimator):  \n",
    "#     \"\"\"An example of classifier\"\"\"\n",
    "\n",
    "#     def __init__(self, name=None, estimator=None, params={}, x_pred=None, y_true=None, **kwargs):\n",
    "# #         print('INIT', name)\n",
    "#         self.name = name or 'svc' # type(models[model])\n",
    "#         self.estimator = estimator\n",
    "#         self.x_pred = x_pred\n",
    "#         self.y_true = y_true\n",
    "#         self.params = params  # or {}\n",
    "# #         self.model_instance = models[self.name]\n",
    "#         self.initial_params = _models[self.name].get_params()\n",
    "# #         print('PARAMS 1', name, self.name, self.model_instance, self.initial_params)\n",
    "# #         params.update()\n",
    "#         self.model_instance = type(_models[self.name])(**self.params)\n",
    "    \n",
    "# #     def get_model_instance(self):\n",
    "# #         return type(self.model_instance)(**self.params)\n",
    "    \n",
    "#     @property\n",
    "#     def name(self):\n",
    "#         return self._name\n",
    "    \n",
    "#     @name.setter\n",
    "#     def name(self, value):\n",
    "# #         print('SET NAME', value)\n",
    "#         self._name = value\n",
    "#         params = _models[self.name].get_params()  # getattr(self, 'initial_params', {}).copy() \n",
    "#         params.update(getattr(self, 'params', {}))\n",
    "# #         print('PARAMS 2', value, getattr(self, 'name', None), getattr(self, 'model_instance', None), params)\n",
    "#         self.model_instance = type(_models[self._name])(**params)\n",
    "# #         self.model_instance = type(_models[self._name])(**getattr(self, 'params', {}))\n",
    "    \n",
    "#     @property\n",
    "#     def params(self):\n",
    "#         return self._params\n",
    "    \n",
    "#     @params.setter\n",
    "#     def params(self, value):\n",
    "# #         print('SET PARAMS', value)\n",
    "#         self._params = value\n",
    "#         params = getattr(self, 'initial_params', {}).copy()\n",
    "#         params.update(getattr(self, 'params', {}))\n",
    "# #         print('PARAMS 3', value, getattr(self, 'name', None), getattr(self, 'model_instance', None), params)\n",
    "#         self.model_instance = type(_models[self._name])(**params)\n",
    "# #         self.model_instance = type(_models[self._name])(**getattr(self, 'params', {}))\n",
    "# # #         print('SET PARAMS', value)\n",
    "\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "# #         self.get_model_instance().fit(X, y)\n",
    "# #         return self\n",
    "# #         self.model_instance = models[self.name]\n",
    "#         self.model_instance.fit(X, y)\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X, y=None):\n",
    "# #         return self.get_model_instance().predict(X)\n",
    "# #         self.model_instance = models[self.name]\n",
    "#         return list(map(int, self.model_instance.predict(X)))\n",
    "       \n",
    "#     def score(self, X, y=None):\n",
    "# #         print('SCORE', self.model_instance.predict(X).shape, y.shape)\n",
    "#         error = mean_squared_error(self.model_instance.predict(X), y)\n",
    "#         score = 1 / (1 + error)\n",
    "#         print(self.name, score, error)\n",
    "#         print('SCORE', self.name, score, self.model_instance.score(X, y))\n",
    "#         return score\n",
    "    \n",
    "#         error = sum([1 if 0.1 < abs(y1 - y2) else 0 for y1, y2 in zip(self.model_instance.predict(X), y)])/y.shape[0]\n",
    "#         score = 1 / (1 + error)\n",
    "#         print(self.name, score, error)\n",
    "#         return score\n",
    "# #         print('SCORE', X.shape, y.shape)\n",
    "# # #         return self.model_instance.score(X, y)\n",
    "# #         if self.estimator:\n",
    "# #             return self.estimator(self.model_instance)\n",
    "# # #         print('SCORE', self.model_instance)\n",
    "# # #         return accuracy_score(y_pred=self.predict(self.x_pred), y_true=self.y_true)\n",
    "        \n",
    "# # #         print('SCORE', self.model_instance)\n",
    "# # # # #         return self.get_model_instance().score(X, y)\n",
    "# # # #         return self.model_instance.score(X, y)\n",
    "# # #         self.model_instance = models[self.name]\n",
    "# #         return(sum(self.model_instance.predict(X)))\n",
    "# # #         return(sum(self.predict(X))) \n",
    "# # #         return accuracy_score(y_pred=self.predict(self.x_pred), y_true=self.y_true)\n",
    "    \n",
    "# def completor(df):\n",
    "#     df = df[:]\n",
    "#     cats = df.category_name.str.split('/', expand=True)\n",
    "# #     print('<', df.is_copy())\n",
    "# #     df.is_copy = False\n",
    "# #     df.loc[:, 'cat1'] = cats[0]\n",
    "# #     df.loc[:, 'cat2'] = cats[1]\n",
    "# #     df.loc[:, 'cat3'] = cats[2]\n",
    "# #     print('>')\n",
    "#     df = df.drop(['category_name'], axis=1)\n",
    "    \n",
    "#     df = pd.DataFrame(dict(**df.to_dict(), cat1=cats[0], cat2=cats[1], cat3=cats[2]))\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def _execute_analyze_cycle(model, dataframe, *, x=None, y=None, params=None, features=None, matched=False):\n",
    "#     ret = dict()\n",
    "    \n",
    "# #     def sample_selector(x, count=None, *args, **kwargs):\n",
    "# # #         print('SELECTOR', x, args, kwargs)\n",
    "# # #         print('SELECTOR', x, features)\n",
    "# #         print('SAMPLE', x[:10], count, args, kwargs)\n",
    "# # #         if count:\n",
    "# # #             x = x.iloc[:count]\n",
    "# # #         if count:\n",
    "# # #             y = y[:count]\n",
    "# #         return x\n",
    "        \n",
    "# #     def selector(X, *args, **kwargs):\n",
    "#     def feature_selector(x, features=None):\n",
    "# #         print('SELECTOR', x, args, kwargs)\n",
    "# #         print('SELECTOR', x, features)\n",
    "# #         print('FEATURE', type(x), features)\n",
    "#         if features:\n",
    "# #             x = x.loc[:, features]\n",
    "#             x = x.loc[:, features.split(',')]\n",
    "#         return x\n",
    "    \n",
    "#     dataframe = dataframe.dropna()\n",
    "    \n",
    "#     x = x or dataframe.drop(['price'], axis=1)\n",
    "#     y = y or np.ravel(dataframe.loc[:, ['price']])\n",
    "\n",
    "\n",
    "#     preparation = Pipeline(steps = [\n",
    "# #             ('x_selector', FunctionTransformer(sample_selector, validate=False, )),\n",
    "# #             ('y_selector', FunctionTransformer(sample_selector, pass_y=True, validate=False, )),\n",
    "#         ('feature_selector', FunctionTransformer(feature_selector, validate=False)),\n",
    "\n",
    "#         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#         ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "#         ('scaler', StandardScaler()),            \n",
    "# #         ('model', models[model]) if isinstance(model, str) else model,\n",
    "# #         ('model', Router('log_reg', estimator=estimator, x_pred=x_test, y_true=y_true))\n",
    "#     ])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     x_train, x_test, y_train, y_test = train_test_split(\n",
    "#         x,\n",
    "#         y,\n",
    "#         test_size=0.20, \n",
    "#         random_state=42\n",
    "#     )\n",
    "\n",
    "#     y_true = np.ravel(y_test)\n",
    "# #     y_pred = search_cv.predict(x_test)\n",
    "\n",
    "\n",
    "#     _x_train, _x_test, _y_train, _y_test = train_test_split(\n",
    "#         preparation.fit_transform(x),\n",
    "#         y,\n",
    "#         test_size=0.20, \n",
    "#         random_state=42\n",
    "#     )\n",
    "#     _y_true = np.ravel(_y_test)\n",
    "\n",
    "\n",
    "#     def estimator(model):\n",
    "#         print(model)\n",
    "# #         x = preparation.fit_transform(x_test)\n",
    "#         model.fit(_x_train, _y_train)\n",
    "# #         print(model.predict(x).shape, y_true.shape)\n",
    "#         return accuracy_score(y_pred=list(map(int, model.predict(_x_test))), y_true=_y_true)\n",
    "\n",
    "#     pipeline = Pipeline(steps = [\n",
    "#         ('completor', FunctionTransformer(completor, validate=False)),\n",
    "# #             ('x_selector', FunctionTransformer(sample_selector, validate=False, )),\n",
    "# #             ('y_selector', FunctionTransformer(sample_selector, pass_y=True, validate=False, )),\n",
    "#         ('feature_selector', FunctionTransformer(feature_selector, validate=False)),\n",
    "\n",
    "#         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#         ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "#         ('scaler', StandardScaler()),            \n",
    "# #         ('model', models[model]) if isinstance(model, str) else model,\n",
    "#         ('model', Router('log_reg', estimator=None, x_pred=x_test, y_true=y_true))\n",
    "#     ])\n",
    "\n",
    "#     grid = params or {}\n",
    "\n",
    "# #     search_cv = RandomizedSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "#     search_cv = GridSearchCV(pipeline, grid, cv=3, iid=True)             \n",
    "#     search_cv.fit(x_train, np.array(y_train))\n",
    "\n",
    "#     ret['search_cv'] = search_cv\n",
    "\n",
    "#     y_pred = search_cv.predict(x_test)\n",
    "    \n",
    "# #     print('PRED', y_pred)\n",
    "# #     print('TRUE', y_true)\n",
    "\n",
    "#     ret['accuracy_score'] = 0  # accuracy_score(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "#     return ret\n",
    "\n",
    "\n",
    "# grid = {\n",
    "# # #     'model__penalty': ['l2', 'none'],\n",
    "# #     'model__max_iter': [250], # [100, 250, 500, 1000],\n",
    "    \n",
    "# #     'x_selector__kw_args': [{'count': count} for count in [250, 500, 750]],\n",
    "# #     'y_selector__kw_args': [{'count': count} for count in [250, 500, 750]],\n",
    "#     'feature_selector__kw_args': [\n",
    "#         {'features': 'brand_name'}, \n",
    "#         {'features': 'cat1'}, \n",
    "#         {'features': 'cat2'}, \n",
    "#         {'features': 'cat3'},\n",
    "#         {'features': 'shipping'},\n",
    "#     ],\n",
    "#     'model__name': list(_models.keys()),\n",
    "# #     'model__name': ['knei_reg'], # list(models.keys()),  # ['log_reg', 'svc', 'tree'], # [100, 250, 500, 1000],\n",
    "# # #     'model__params': [{'C': C} for C in [1, 10, 100, 1000, 10000]],\n",
    "# #     'model__params': [{'n_neighbors': param} for param in range(2, 6)],\n",
    "# # #     'model__params': [{'max_iter': param} for param in range(100, 1000, 100)],\n",
    "# }\n",
    "\n",
    "# # SVC(kernel='linear', C=100)\n",
    "\n",
    "# # _execute_analyze_cycle(models['log_reg'], df, params=grid)\n",
    "# # data = _execute_analyze_cycle('log_reg', df[:137], params=grid)\n",
    "# # data = _execute_analyze_cycle(prepare_data(1000), df[:137], params=grid)\n",
    "# data = _execute_analyze_cycle('log_reg', prepare_data(300), params=grid)\n",
    "# # prepare_data(N, subcats=True)\n",
    "\n",
    "# # data['search_cv'].cv_results_\n",
    "# # pd.DataFrame(data['search_cv'].cv_results_).loc[:, filter(lambda key: not key.endswith('time'), data['search_cv'].cv_results_.keys())]\n",
    "# # pd.DataFrame(data['search_cv'].cv_results_).groupby(['param_model__name', 'param_feature_selector__kw_args']).sum()['mean_test_score'].unstack().plot.bar()\n",
    "# pd.DataFrame(data['search_cv'].cv_results_)\n",
    "# # {k:(v['features'][0] if k == 'param_feature_selector__kw_args' else v) for k, v in data['search_cv'].cv_results_.items()}\n",
    "# # {k:print(k, v) for k, v in data['search_cv'].cv_results_.items()}\n",
    "# # data = {k:[(vv['features'][0] if k == 'param_feature_selector__kw_args' else vv) for vv in v] for k, v in data['search_cv'].cv_results_.items()}\n",
    "\n",
    "# data = data['search_cv'].cv_results_\n",
    "\n",
    "# data\n",
    "\n",
    "# params = list(dict(data)['params'])[0]\n",
    "# # param_name ='param_model__params'\n",
    "# param_name = 'param_' + list(params.keys())[0]\n",
    "# param_name\n",
    "\n",
    "# data = {k:[(list(vv.values())[0] if k == param_name else vv) for vv in v] for k, v in data.items()}\n",
    "# data = pd.DataFrame(data)\n",
    "# data\n",
    "\n",
    "# # params = data.iloc[0]['params']\n",
    "# # params.pop('feature_selector__kw_args')\n",
    "# params.pop('model__name')\n",
    "\n",
    "# # # # data = pd.DataFrame({k:(v['features'][0] if k == 'param_feature_selector__kw_args' else v) for k, v in data['search_cv'].cv_results_.items()})\n",
    "# # # data.groupby(['param_model__name', 'param_feature_selector__kw_args']).sum()['mean_test_score'].unstack().plot.bar()\n",
    "# # param_name = 'param_' + list(params.keys())[0]  # 'param_model__params'\n",
    "# data.groupby(['param_model__name', param_name]).sum()['mean_test_score'].unstack().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# by_model_and_feature.groupby(['model', 'feature']).sum()['accuracy_score'].unstack().plot.bar()# (x='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # (0.1, Pipeline(memory=None,\n",
    "# #           steps=[('model',\n",
    "# #                   XGBClassifier(base_score=0.5, booster='gbtree',\n",
    "# #                                 colsample_bylevel=1, colsample_bynode=1,\n",
    "# #                                 colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
    "# #                                 max_delta_step=0, max_depth=3,\n",
    "# #                                 min_child_weight=1, missing=None,\n",
    "# #                                 n_estimators=25, n_jobs=1, nthread=None,\n",
    "# #                                 objective='multi:softprob', random_state=0,\n",
    "# #                                 reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "# #                                 seed=None, silent=None, subsample=1,\n",
    "# #                                 verbosity=1))],\n",
    "# #           verbose=False))\n",
    "\n",
    "# df = prepare_data(300)\n",
    "\n",
    "# for n in range(1, 11, 5):\n",
    "# # for n in range(1, 206, 5):\n",
    "#     param_grid = {\n",
    "#     #     'max_depth': [2, 3, 4],\n",
    "#     #     'model__n_estimators': range(25, 225, 25), # [25, 50, 100],\n",
    "#     #     'model__n_estimators': [50], # range(25, 225, 25), # [25, 50, 100],\n",
    "#         'model__n_estimators': [n], # range(25, 225, 25), # [25, 50, 100],\n",
    "#     #     'learning_rate': [0.01, 0.025]\n",
    "#     }\n",
    "#     # xgb = randomized_cv(xgb.XGBClassifier(), param_grid, x_train, y_train)\n",
    "#     score = execute_analyze_cycle(\n",
    "#         xgb.XGBClassifier(),\n",
    "#     #     df.drop(['price'], axis=1), \n",
    "#     #     np.ravel(df.loc[:, ['price']]),\n",
    "#         df,\n",
    "#         params=param_grid,\n",
    "#     )\n",
    "\n",
    "#     display(n, score['accuracy_score'], score['best_estimator'])\n",
    "\n",
    "# # score['accuracy_score'], score['best_estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_train, x_test, y_train, y_test = train_test_split(\n",
    "# train_test_split(\n",
    "#     df.drop(['price'], axis=1), \n",
    "#     np.ravel(df.loc[:, ['price']]),\n",
    "#     test_size=0.20, \n",
    "#     random_state=42\n",
    "# )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# # log_reg = LogisticRegression ()\n",
    "# # log_reg . fit (X , y )\n",
    "# # y_proba = log_reg . predict_proba ( X_new )\n",
    "\n",
    "# pipeline = Pipeline(steps = [\n",
    "# # #     ('imputer', SimpleImputer()),\n",
    "# #     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "# #     ('scaler', StandardScaler()),\n",
    "# # #     ('pca', PCA(n_components=2)),\n",
    "# # #         ('pca', PCA()),\n",
    "# #     ('log_reg', log_reg),\n",
    "#     ('log_reg', LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)),\n",
    "# ])\n",
    "\n",
    "# # display(X_train.info())\n",
    "# # display(y_train.info())\n",
    "\n",
    "# # # pipeline.fit(df.drop(['price'], axis=1), df.loc[:, ['price']])\n",
    "# # pipeline.fit(x_train, y_train)\n",
    "# # # y_pred = pipeline.steps[-1][-1].predict(df_test)\n",
    "# # y_pred = pipeline.predict(x_train)\n",
    "# # # y_pred = pipeline.predict(df_test)\n",
    "# # # display(y_pred)\n",
    "\n",
    "# # X_prepared = pipeline.fit_transform(df)\n",
    "# # X_prepared.shape, X_prepared\n",
    "# # X_prepared[0]\n",
    "\n",
    "# grid = {\n",
    "#     'log_reg__penalty': ['l2']\n",
    "# }\n",
    "\n",
    "# better = GridSearchCV(pipeline, grid, cv=3, iid=True)     \n",
    "# # display(y_train.shape)\n",
    "# better.fit(np.array(x_train), np.ravel(np.array(y_train)))  # .reshape(1, y_train.shape[1]), )\n",
    "\n",
    "# better.best_estimator_\n",
    "\n",
    "# display(better.best_score_)\n",
    "\n",
    "# accuracy_score(y_pred = better.predict(x_test), y_true = np.ravel(y_test))\n",
    "# # accuracy_score(y_pred = better.predict(np.array(test[features])), y_true = test.Survived)\n",
    "\n",
    "# {'best_score_': 0.09045226130653267, 'accuracy_score': 0.1}\n",
    "# {'best_score_': 0.10569105691056911, 'accuracy_score': 0.03225806451612903}\n",
    "# {'best_score_': 0.10569105691056911, 'accuracy_score': 0.03225806451612903}\n",
    "# {'best_score_': 0.09547738693467336, 'accuracy_score': 0.08}\n",
    "# {'best_score_': 0.10569105691056911, 'accuracy_score': 0.03225806451612903}\n",
    "\n",
    "df = prepare_data(1250)\n",
    "df = df.dropna()\n",
    "\n",
    "df['brand_id'] = pd.Categorical(df['brand_name']).codes\n",
    "\n",
    "grid = {\n",
    "#     'model__penalty': ['l2', 'none'],\n",
    "    'model__max_iter': [250], # [100, 250, 500, 1000],\n",
    "#     'x_selector__kw_args': [{'count': count} for count in [250, 500, 750]],\n",
    "#     'y_selector__kw_args': [{'count': count} for count in [250, 500, 750]],\n",
    "    'feature_selector__kw_args': [\n",
    "        {'features': ['brand_name']}, \n",
    "#         {'features': ['cat1']}, \n",
    "#         {'features': ['cat2']}, \n",
    "#         {'features': ['cat3']}\n",
    "    ],\n",
    "}\n",
    "\n",
    "# data = execute_analyze_cycle(LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000), df)\n",
    "# data = execute_analyze_cycle(LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000), df)\n",
    "# data = execute_analyze_cycle(LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000), df, \n",
    "#   features=['brand_name'])\n",
    "# data = execute_analyze_cycle(models['log_reg'], df, params=grid, features=['brand_name'])\n",
    "data = execute_analyze_cycle(models['log_reg'], df, params=grid)\n",
    "\n",
    "\n",
    "# data = execute_analyze_cycle(\n",
    "#     LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000), \n",
    "#     df.drop(['price'], axis=1), \n",
    "#     np.ravel(df.loc[:, ['price']]),\n",
    "# )\n",
    "\n",
    "data['best_score'], data['accuracy_score']\n",
    "# (0.0668103448275862, 0.05128205128205128)\n",
    "# (0.05818965517241379, 0.05982905982905983)\n",
    "\n",
    "data['best_estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[:2]\n",
    "# pd.DataFrame(data['search_cv'].cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# # lin_svm = SVC(kernel='linear', C=100).fit(x_train, y_train)\n",
    "\n",
    "# pipeline = Pipeline(steps = [\n",
    "# #     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "# #     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "# #     ('scaler',StandardScaler()),\n",
    "#     ('svc', SVC(kernel='linear', C=100))\n",
    "# ])\n",
    "# grid = {\n",
    "    \n",
    "# }\n",
    "# better = GridSearchCV(pipeline, grid, cv=3, iid=True)     \n",
    "# better.fit(np.array(x_train), np.array(y_train))\n",
    "\n",
    "# better.best_estimator_\n",
    "\n",
    "# display(better.best_score_)\n",
    "\n",
    "# accuracy_score(y_pred = better.predict(x_test), y_true = np.ravel(y_test))\n",
    "\n",
    "# df = prepare_data(250)\n",
    "\n",
    "# data = execute_analyze_cycle(\n",
    "#     SVC(kernel='linear', C=100),\n",
    "#     df,\n",
    "# )\n",
    "\n",
    "# data['best_score'], data['accuracy_score']\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for i in [1]:\n",
    "#     scores[i] = execute_mutly_analyze_cycles(range(250, 1250, 250), SVC(kernel='linear', C=100), trace=True)\n",
    "    scores[i] = execute_mutly_analyze_cycles(range(250, 500, 250), SVC(kernel='linear', C=100), trace=True)\n",
    "\n",
    "display(scores[1])\n",
    "# display(scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df['brand_id'] = pd.Categorical(df['brand_name']).codes\n",
    "\n",
    "# execute_analyze_cycle(\n",
    "# #     LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000), \n",
    "#     DecisionTreeClassifier(),\n",
    "#     df.drop(['price'], axis=1).loc[:, ['brand_id']], \n",
    "#     np.ravel(df.loc[:, ['price']]),\n",
    "# )\n",
    "\n",
    "# # {'best_score_': 0.05025125628140704, 'accuracy_score': 0.04}\n",
    "# # {'best_score_': 0.05025125628140704, 'accuracy_score': 0.06}\n",
    "\n",
    "# # pipeline = Pipeline(steps = [\n",
    "# # #     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "# # #     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "# # #     ('scaler',StandardScaler()),\n",
    "# #     ('tree', DecisionTreeClassifier())\n",
    "# # ])\n",
    "# # grid = {\n",
    "    \n",
    "# # }\n",
    "# # better = GridSearchCV(pipeline, grid, cv=3, iid=True)     \n",
    "# # better.fit(np.array(x_train), np.array(y_train))\n",
    "\n",
    "# # better.best_estimator_\n",
    "\n",
    "# # display(better.best_score_)\n",
    "\n",
    "# # accuracy_score(y_pred = better.predict(x_test), y_true = np.ravel(y_test))\n",
    "\n",
    "\n",
    "# {'best_score_': 0.05025125628140704, 'accuracy_score': 0.04}\n",
    "# {'best_score_': 0.05025125628140704, 'accuracy_score': 0.06}\n",
    "# 0.06238279784973122\n",
    "\n",
    "\n",
    "# df = prepare_data(17000)\n",
    "\n",
    "# # display(df[3700:3750])\n",
    "# display(df[3780:3783])\n",
    "\n",
    "scores = pd.DataFrame({'size': [], 'best_score': [], 'accuracy_score': []})\n",
    "display(scores)\n",
    "# for i, N in enumerate(range(1000, 10000, 1000)):\n",
    "# for N in [3000, 3099, 3299, 3499, 3699, 3999, 4200]:\n",
    "# for N in [3780, 3830, 3960, 3390, 3999, 4410]:\n",
    "# for N in range(3780, 3810):\n",
    "for i, N in enumerate([9999]):\n",
    "#     print(N)\n",
    "    \n",
    "    df = prepare_data(N)\n",
    "    df = df.dropna()\n",
    "\n",
    "    df['brand_id'] = pd.Categorical(df['brand_name']).codes\n",
    "\n",
    "    score = execute_analyze_cycle(\n",
    "        DecisionTreeClassifier(),\n",
    "        df,\n",
    "#         df.drop(['price'], axis=1), \n",
    "# #                 df.drop(['price'], axis=1).loc[:, ['brand_id']], \n",
    "#         np.ravel(df.loc[:, ['price']]),\n",
    "        matched=True,\n",
    "        features=['brand_name'],\n",
    "    )\n",
    "        \n",
    "    \n",
    "#     score = execute_analyze_cycle(\n",
    "#         DecisionTreeClassifier(),\n",
    "#         df.drop(['price'], axis=1), \n",
    "# #                 df.drop(['price'], axis=1).loc[:, ['brand_id']], \n",
    "#         np.ravel(df.loc[:, ['price']]),\n",
    "#         matched=True,\n",
    "#     )\n",
    "    \n",
    "    scores.loc[i] = (N, score['best_score'], score['accuracy_score'])\n",
    "    \n",
    "#     display(score['accuracy_score'])\n",
    "\n",
    "# scores.plot.plot(x='size', y='score')\n",
    "scores\n",
    "# size\tbest_score\taccuracy_score\n",
    "# 0\t9999.00\t0.05\t0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# scores.plot(x='size', y='score')\n",
    "\n",
    "# score\n",
    "# x_test, y_pred, y_true = itemgetter('x_test', 'y_pred', 'y_true')(score)\n",
    "# display(df[df['train_id'].isin(x_test[y_pred == y_true]['train_id'])])\n",
    "# y_pred[y_pred == y_true], y_true[y_pred == y_true], x_test[y_pred == y_true]['train_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = prepare_data(300)\n",
    "\n",
    "data = execute_analyze_cycle(\n",
    "    [\n",
    "        LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000),\n",
    "        SVC(kernel='linear', C=100),\n",
    "        DecisionTreeClassifier()\n",
    "    ], \n",
    "    df\n",
    "#     LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000), df, features=['brand_name']\n",
    ")\n",
    "\n",
    "data['best_score'], data['accuracy_score'], data['accuracy_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['predictions'], data['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Провести простые преобразования признаков и добавление простых признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### разделить category_name на уровни"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### посмотреть на наличие числовых значений в описании и имени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание\n",
    "1. Сделать baseline submission\n",
    "  * Исследовать признак price. \n",
    "  * Исследовать признак price в зависимости от brand_name или других признаков\n",
    "2. Реализовать цикл анализа\n",
    "  * признаки -> модель -> настройка параметров -> лучшая модель и ее значение метрики качества на кросс-валидации\n",
    "3. Провести простые преобразования признаков и добавление простых признаков\n",
    "  * разделить category_name на уровни\n",
    "  * посмотреть на наличие числовых значений в описании и имени\n",
    "  * ...\n",
    "4. Составить план по применению нескольких моделей на разных признаках\n",
    "  * спроектируйте эксперимент. Нужно заранее спланировать порядок перебора признаков и моделей. Потом только писать код. Обратный порядок вызывает необходимость переписывать существующий код, это трата времени\n",
    "5. Просмотреть 1-5 kernel на kaggle. (только в таком порядке. сначала работаем самостоятельно, потом смотрим идеи других. при появлении опыта можно сразу начинать с них)\n",
    "6. Скорректировать план\n",
    "  * убрать пункты, которые кажутся неудачными\n",
    "  * добавить идеи из kernel, кажущиеся удачными\n",
    "7. Построить модель, выбрать лучшую\n",
    "8. Построить ансамбль, настрить парамертры. Сравнить с другими моделями.\n",
    "9. Применить и засабмитить лучшую на cv модель\n",
    "10. Прислать блокнот и свой ник в лидерборде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
